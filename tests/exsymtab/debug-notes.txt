TODO
====

- Write a struct declaration sharing test that uses non-int types, like float or char[10]
- Write tests for extended symbol table traps discussed on May 1
- variable interpolation: function unary() in tccgen.c
- may need to clear the .c field of each sym's type after each context's use
- check for pad entries using pad_findmy_pv or pad_findmy_pvn, then get the sv using PAD_SV

====

break tcc_compile_string
break tcc_relocate_ex
# This is where the interesting parsing stuff happens
break block
break gfunc_call
lib_path=../..

# Probably want to break on decl0

# Try this to break within next():
break /home/dcmertens/projects/tinycc/tccpp.c:3017

# Otherwise, after the first breakpoint above gets tripped, add this:
break next_nomacro_spc

### When I call tcc_add_symbol... ###

# ... I get to relocate_syms() ...
# What is sh_num? ANS: it is SHN_ABS, which is 0xfff1.
# How does it compare to SHN_LORESERVE == 0xff00? ANS: it is greater.
# If it were lower, it would be changed, so in relocate_syms(), the address is not changed.



line 2329 of libtcc.c
int flagless_original = curr_Def->v & ~(SYM_STRUCT | SYM_FIELD | SYM_FIRST_ANOM);



#### April 27 2014

It's been a while since I last looked at this code.

# Catching up

As of right now, there are tests under tests/exsymtab/, and tests 01, and 20 pass. 30 does
not pass. Neither does the overly complex 09, which is copied and unmodified in 40. 50
does not pass either, and a printout of the address of the two functions "fib" and
"fib_of_5" show that they have the *same* address, which is wrong.

To proceed, and to lose as little ground between steps as possible, I am going to proceed
systematically. I will begin by focusing on test 20.

# Getting oriented with test 20

Test 20 defines a struct in one context, copies that context's symbol table, applies that
symbol table to another context, and tries to compile code that uses the struct
definition. The struct "constructor" can be obtained from the first context after
compilation and it runs without trouble. It is possible to ensure it works by manually
unpacking the struct (treating it like a two-element array).

The error message when the second compiler runs into trouble is 
  <string>:3: error: field not found: x
This means that the compiler does not have any trouble with the statement on the previous
line:
  struct point * pt = pt_p;
That suggests that it is able to search for the symbol called "point" in the
extended symbol table and find it. To be sure of this, I added the following lines to the
extended symbol table search found in the vicinity of line 2280 in tccpp.c:
  if (!memcmp("point", p1, len)) printf("Looking for a point with an extended symbol table callback\n");
  				ts = tcc_state->symtab_name_callback(
  					p1, len, tcc_state->symtab_callback_data, 0);
  if (!memcmp("point", p1, len) && ts) printf("Found a point symbol! Returning %d\n", ts);
These if statements print during the compilation stage before the second compilation
croaks. So we can be sure that the system finds the symbol. Furthermore, the printout of
the symbol number suggests that the return value is sensible.

At present, test 20 shows the following problem:

  ############################################################
  # Extended symbol table entries are not correctly queried. #
  ############################################################

How might we test this? I would have to locate where struct data offsets are queried and
then print out some diagnostics whenever we try to access data from the "point" struct.
Where does this occur? Thanks to ack, I found that this error is printed out on line 3954
of tccgen.c

In the vicinity of this error, the type of the struct that we are accessing is stored in
vtop. vtop is an SValue, a struct defined on line 319 of tcc.h. vtop->type is a CType,
a struct defined on line 298 of the same header file. Judging by the usage, the type's
ref slot (vtop->type.ref) is a Sym linked list. The Sym struct is defined on line 329 of
tcc.h. The v field of that struct is what we're looking for, and is the symbol token of
the Sym in question.

The while loop in tccgen.c that precedes the current error checks if any of the members
of the linked list have the same symbol token as the current symbol token, mostly. The
actual symbol token compared is the current symbol token OR'd with SYM_FIELD. Also, the
search through the linked list starts with the *second* element of the list, not the
first. Here are some important questions worth asking:

  Q: What is contained in the first element of the list?
  Q: When the elements of the list are copied in the symbol table copy function, are
     they copied with the correct flags? [ans line 97]
  Q: Is the token symbol for "x" correctly retrieved for this situation?

I can most easily answer the second question first. The symbol copying is contained in
libtcc.c in the vicinity of line 2220. In particular, all flags are split off of the token
symbol in the vicinity of line 2210, then added back after the arithmetic. Every single
symbol is copied and its symbol token updated in this one spot. The ref pointer is
copied further down, in such a way as to refer to the new, copied symbol token.

I now suspect, strongly, that the problem is that the flags are being mishandled
somewhere. Line 2503 of libtcc.c copies over the flags, which seems odd since the
tokenizer does not understand context and cannot differentiate between struct names,
field names, etc. I do not understand the role of these. At any rate, the value SYM_FILED
is OR'd with the symbol token before the comparison, so the return value of the
name-to-token function should probably *not* have that value. Either that, or all such
flags get stripped out at some point:

  Q: Do extra flags such as SYM_FIELD get stripped out from the symbol token after it is
     returned?



#### April 28 2014

New investigations to resolve the questions asked last night. It turns out that the token
for the "x" struct field is never identified in the extended symbol code. This suggests
that there may be a mistake in the code that searches through extended symbols.

To reach this observation, I added the following diagnostic to tccgen.c, line 3954:

 { printf("Could not find token %x\n", tok);
                tcc_error("field not found: %s",  get_tok_str(tok & ~SYM_FIELD, NULL));
 }


#### April 30 2014

Continuing with my previous line of inquiry, I decided to look more closely at the
preprocessor parsing of single-character tokens, such as "x". How are TokenSym structs
allocated for these? Do they get copied by the symbol table copier?

# TokenSym for x?

To answer that question, I look into the token symbol copying in libtcc. Adding this line
to the copy function:

  printf("Just copied token with string %s\n", tok_sym->str);

lets me check for all tokens that get copied. Do we see "x"?

Many, many, many tokens are created, more than I care to know. They are all probably
preprocessor macros, and I should probably find some way to avoid compiling them each
time the compiler gets invoked. At any rate, we *do* have an "x" token, along with a "y"
token.

Why is this odd? It appears that the lookup-by-name code in test_setup.h can find the
"point" and "pt" token symbols, but is unable to find the [x] token symbol.

# TokenSym search

This sugests that the lookup-by-name code is either broken or not called for "x". To
check this, I added this line to the token lookup-by-name function in test_setup.h:

  printf("Looking up token with name [%s]\n", name_to_find);

I also removed diagnostic output to make the output easier to digest. The output is what
I would have expected:

  Looking up token with name [sq_distance_to_pt]
  Looking up token with name [pt_p]
  Looking up token with name [point]
  Looking up token with name [pt]
  Looking up token with name [pt]
  Looking for an x with an extended symbol table callback
  Looking up token with name [x]
  Looking up token with name [sq_distance_to_pt???]

Thus we are indeed asked to find "x", and I am confident that the "x" tokenSym is indeed
copied to the extended symbol table. There must be an issue with the lookup procedure.

To check if the lookup procedure is faulty, I added a printout to the lookup function that
displays all token names that failed, either because the names didn't match or because the
TokenSym is "not shareable".

And there we see it, hidden among scores of similar lines:

  TokenSym with name [x] does not match or is not shareable

which suggests that the TokenSym for "x" is not shareable. This is odd since the TokenSym
for "point" is shareable, apparently. My next step, then, is to test if I can make this
field shareable.

# Shareability

There are two tests I can perform to see if I can get this to work. The first is to try
using a longer name, in case single-letter tokens behave oddly. The second is to look
more closely at the logic for sharable tokens. I will begin with the first idea. To do
this, I simply rename the fields of the struct from "x" and "y" to "xval" and "yval". For
good measure I also changed the names of the arguments to new_point().

Once again, I get this message:

  Looking up token [xval]
  Could not find token 1000046f

So it appears that the members of structs are not shareable, and so lookup_by_name fails
to return the TokenSym for this field.


#### May 1 2014

After looking at the logic for tcc_tokensym_is_shareable, I think I understand more
clearly that the TokenSym's fields that are checked in this function would point to
globally visible things. The function might be better named tcc_tokensym_has_globals.

At any rate, I decided to remove the "is_shareable" check from the lookup_by_name
function in test_setup.h to see if the struct code will properly compile.

HOLY SHIT IT WORKED!!! HURRAY!!! TEST 30 NOW PASSES!!!

And just to make sure that things aren't stupid, I decided to change one of the variable
names back to a single character, and to reuse that token repeatedly. This way single
character tokens as well as repeated use of the same extended token in different contexts
get tested.

Yes, it all works just as it should. :-D

# Potential TokenSym lookup issues

I have a worry about a potential trap in TokenSym lookup by name. This would only reveal
itself when using three contexts.

In context #1, I write a function called "foo". This is a globally accessible thing, so it
gets stored in the TokenSym's function slot. This TokenSym is copied to an extended symbol
table.

In context #2, I include the extended symbol table from context #1 and write a macro
called "foo". The token "foo" is associated with the TokenSym in the first context's
extended symbol table, so that TokenSym (and tok reference) is returned. This globally
accessible thing gets stored in the *original* TokenSym's define slot! Furthermore, when
the extended symbol table is made from context #2, it does not include a TokenSym for
this symbol! The symbol itself should be in this context's list of symbols, so it will be
copied. Potential problems include:

  1) The original TokenSym's direct pointer to the symbol itself will not be updated with
     the creation of the extended symbol table. This will be a potential memory leak.
  2) If context #2 is applied to a later context, the lookup_by_name function will have to
     know to look for tokens in context #1, or it won't find "foo" defined as a macro.
  3) If context #1 is applied to a later context, the "foo" macro slot will already be
     taken, even though the macro was *not* defined in context #1. Thus if we want to have
     two contexts (say #2 and #3) that morph the behavior of the function call with two
     different macro wrappers, we will likely run into trouble.

I need to write tests for all of these, but would like to move to other things first.

# Function declarations

I now turn my attention to shared functions. The test file for this material was test 40.
I have substantially refactored it to provide cleaner tests and testing framework. I have
also renamed it to describe better what it tests.

Currently, it appears that the function declaration itself works correctly. However, as
the test file now clearly indicates, functions from other contexts do not appear to have
the correct address, something I struck upon when examining this problem back in December.
In contrast to December, I now have an automated test that can reliably report this
failure without inducing a segmentation fault.

# Vtables

A test that falls somewhere between struct declaration sharing and function sharing is
function declaration sharing. I can achieve this test most simply by preparing a context
that declares and can assemble a vtable. I can then call the assembly function in the
test code itself, and hand off the tested struct to a second context, which can simply
work with the results. Thus is born test 35.

Unfortunately, test 35 behaves very strangely. Any direct access to the function address
of the struct causes the *compiler* to segfault. To see this, instead of this (erroneous)
definition for 

  void * distance_func_ptr(struct point * pt) {
    return pt->x;
  }

try this one:

  void * distance_func_ptr(struct point * pt) {
    return pt->squared_distance;
  }

The compiler croaks. Dereferencing an integer in a struct works, but apparently not a
function pointer.

# Non-integer struct members

We now reach an interesting question: are ints just a special case? What if I used double
or float members in the struct? Would the code still compile?


#### May 4, 2014

I realized that a less complicated test for functions would be to compile a function
declaration, then share it. This avoids the actual function compilation step and any
associated headache with that. For this, I moved the current test 40 to number 45, and
created test 40-two-contexts-func-decl-share.c. In this test, we have a pre-compiled
function (written directly into the test source code). The declaration for this function
is in the first while a use of that function is in the second. This is essentially a test
of whether simple .h precompilation can work.

IT WORKS! HURRAY!

This means that if I have a C library that has already been compiled, and I then parse a
header for that file, the parsed header context can be effectively shared! Awesome! It
also suggests that if I (1) declare a set of functions in one compilation context, (2)
compile them in a second, and (3) invoke them in a third, then it should also work. Hmm...

Test 42 tests the above idea. Test 45, in contrast, simply compiles a function and then
shares the context that includes the function compilation. In test 42, I have three three
contexts, not just two.

IT WORKS AGAIN! HURRAY!

So this means that I am actually very close to having everything working. We see that if a
block of code is filled with declarations, such as struct declarations and the like, then
I can share it. The problem arises when I have definitions. I suspect that this is just as
much a problem when sharing global variables, since in some sense a compiled function is a
global thing.

In particular, if I really want to have a set of functions or globals available, I can
add (context-specific) global variables and function *pointers*, write an initialization
function for the compiled context that sets those the the appropriate values, and finally
call that initialization function just after the code block has run.

This approach, however, seems rather hackish to me. I would rather simply get the
definitions to work out-of-the-box.

I strongly suspect that this has to do with the nature of the symbol. That is, I suspect
that the originally compiled symbol has a flag indicating that it is a strongly linked
value, which is appropriate for the newly compiled code block. However, when it is used in
any later block, the symbol must be marked as weakly linked. This is my hunch...

To test this hunch, I will try weakening all symbols that get *copied* to the extended
symbol table, by ORing VT_WEAK to the symbol's type.t field (roughly line 2250 in
libtcc.c). With this change, I then need to see if all of the previously passing tests
still pass, and if test 45 suddenly passes, too.

Unfortunately, this change broke test 30 and 35, and did not make test 45 pass. There must
be something else going on.

What if I go the other route, and ensure that the offset (i.e. the symbol's c field) is
zero? This is, ostensibly, the offset of the symbol in the elf symbol table. If the offset
is zero, it should indicate that the symbol has not yet been allocated. Line 460 of
libtcc.c seems to suggest this could work. To find out if this'll work, I will change the
assignment of the .c field in the symbol table copy code so that it is zero. Looking over
how the .c field is used, this seems like it must be far to blunt a tool. However, I will
try it and see if it makes things work...

It sorta works! Not entirely, but somewhat. The weak linkage breaks things, but the
combination makes test 45 pass. Now we have the simple question: is weak linkage
necessary? To answer this question, I simply remove the ORed part of the symbol copy that
I inserted earlier.

Hurray! Now the 40-series tests pass. Unfortunately, the 30-series tests fail, which means
that I need to only set the c field very selectively. Reading through tccgen.c, it looks
as though the c field can contain things such as pointer offsets (for struct members) and
string lengths, among other things. So, I should only set it to zero for very specific
symbols, and otherwise I should copy it.

For a first test, I will see if checking the symbol's type.t field is sufficient. It seems
plausible that this will have the right information.

HURRAY! IT WORKS NOW!

Well, mostly. I just realized that test 35 was hobbled so that it always failed, but at
least it always completed without segfaulting. Now, if I unhobble test 35, it segfaults
for reasons I do not yet understand. This means that vtable declarations do not yet work
correctly. However, all of the other functionality, including function declaration
sharing, now works! Awesome!

More work lies ahead, but these are major milestones! :-D

#### May 16, 2014

I have spent quite a bit of time working on the Perl bindings to this library. I kept
encountering trouble, but they gave me enough insight to dig back into the libtcc side of
things. I finally figured out that anonymous symbols are not copied to the symbol table,
and that they had to be copied on their own. I figured out the intricacies of what
information is used in which context, and only copying the right stuff at the right time.
Now, I am pretty sure that function signatures, even complex ones, are copied correctly.

As such, now the un-hobbled test 35 passes! HURRAY!!!!

My bindings for libperl still do not work, but I think I have made significant strides on
getting the tcc symbol table copying up and running.

#### Fall, 2014

Been a little while. Let's see if I can figure out where I left off.

I remember that some segfaults in C::Blocks inspired a set of tests, specifically those in
test 28. My other recent commits before putting the project aside include test 51 and 70.
(Test 29 is identical to 28; I'm not quite sure what I had intended to do with that.)

Test 28 segfaults. It creates a context in which it #defines macros. It creates a second
context that consumes the first by creating new macros that depend upon those defined in
the first. Finally, it creates a third context that uses the macros defined in the second
context. (This is analogous to a C::Blocks-based library that creates some macros which
themselves use the Perl C API, implemented using macros.) The segfault occurs during the
compilation of the third context.

Test 51 passes. It defines a function in one context and uses it in later contexts
without declaring it. This may just be tcc defaulting to integer inputs and outputs for
unknown functions, or it may indicate that the function declaration is being properly
shared.

Test 70 fails three of its 67 tests. The three failing tests all occur with the c field
of the function Sym, which is expected to be zero, but is set to 3 after being used in a
dependent context. I seem to recall that the value of c means that the definition is
given externally, which means that the compiler doesn't look up the definition's function
pointer address. I'd prefer if the compiler looked it up; otherwise it's one more thing
that the compilation manager has to wrestle with.

So I paused work on this project with two open questions: (1) Why does deep macro use
lead to segfaults? (2) Why does a consuming context modify the c field of a function Sym?

#### September 4, 2014

To help answer these questions, I created a new make file extension, .gdb. Now when I try
to make a test and set the extension to .gdb, it'll compile and run the executable under
gdb.

Doing so revealed the following problem with test 28:

  Program received signal EXC_BAD_ACCESS, Could not access memory.
  Reason: KERN_INVALID_ADDRESS at address: 0x0000000000000000
  pstrcpy (buf=0x7fff5fbfe62f " ", buf_size=1025, s=0x0) at libtcc.c:144
  144	            c = *s++;

#### September 5, 2014

Hmmm, that's a string copy starting from a null address. Here's the stack trace:

#0  pstrcpy (buf=0x7fff5fbfe62f " ", buf_size=1025, s=0x0) at libtcc.c:144
#1  0x0000000100008af7 in TOK_GET [inlined] () at /Users/dcmertens/projects/Perl/tinycc/tccpp.c:1015
#2  0x0000000100008af7 in macro_is_equal [inlined] () at /Users/dcmertens/projects/Perl/tinycc/tccpp.c:1016
#3  0x0000000100008af7 in define_push (v=1606411823, macro_type=1606412896, str=0x7fff5fbfea60, first_arg=0x7fff5fbfea60) at tccpp.c:1029
#4  0x000000010000ac6a in parse_define () at tccpp.c:1294
#5  0x0000000100009b90 in is_space [inlined] () at /Users/dcmertens/projects/Perl/tinycc/tcc.h:1412
#6  0x0000000100009b90 in preprocess (is_bof=1606415152) at tccpp.c:2580
#7  0x000000010000b338 in next_nomacro1 () at tccpp.c:2216
#8  0x0000000100008ed5 in next_nomacro [inlined] () at /Users/dcmertens/projects/Perl/tinycc/tccpp.c:2579
#9  0x0000000100008ed5 in next () at tccpp.c:3016
#10 0x00000001000035ca in tcc_compile (s1=0x100802c00) at libtcc.c:794
#11 0x0000000100003be3 in tcc_compile_string (s=0x100802c00, str=0x100039458 "#define add_foo_and_var_to(val) (foo + add_var_to(val))\n") at libtcc.c:849
#12 0x0000000100001197 in main (argc=2, argv=0x7fff5fbff992) at 28-three-contexts-intertwined-preprocessor-macro.c:98

I'm 99% certain that the copy from the null address is due to now knowing where to find
add_var_to().

#### September 6, 2014

The segmentation fault was due to an array dereference, in which the referent was the
null pointer. Looking back up the process, I figured the null pointer probably came from
get_tok_str. New warnings, just added, confirm this hunch.

The offset, 0x7ffffb96, looks like a large constant minus a special number. In fact,
0x80000000 - 0x7ffffb96 is 1130 (base-ten), which I believe is the value of tok_start
that is passed to copy_extended_symtab!! That means there's an errant subtraction in the
symtab copy routine.

It looks like there are two errant lines in the copy routine. I have adjusted for them,
leading to a compile time error:

  Re-defining an extended macro definition is not allowed; #undef first

I wrote that one. :-)

The problem now seems to be with define_push. I am not entirely sure what is leading to
the redefinition... it should just use the already present preprocessor token.

Looking closer, I realize that I should only issue this error when the new macro
definition is different from the original... and in the case of test 28, it is THE SAME.
Thus, if it is the same, and it is an extended preprocessor, I can just return from that
push! Perfect!

Now Test 28 runs all the way through the third! Hurray!

...

No, I was wrong. There was a typo in the test suite, fixed now. And as a result, the third
test of the suite does *not* complete. Instead, I get the compile time warnings and error

  ok 1 - First code string compiled and relocated fine
  <string>:2: warning: exsymtab copy found extended token but no preprocessor symbol for "val" (50000001)
  ok 2 - Second code string compiled and relocated fine
  <string>:4: error: macro 'add_foo_and_var_to' used with too many args

That warning has been around for quite a long time, but it didn't really lead to trouble
in a sensible way until now, finally. The macro function is defined with one argument, and
it gets called with one argument. Clearly, something is going on with the "val" token,
and it's leading to this error, somehow.

The warning is issued by get_new_deftab_pointer, in libtcc.c, indicating that we are
trying to copy an extended table for preprocessor defines, yet the TokenSym struct for the
given token has no Sym object in the sym_define slot.

However, it seems quite plausible that this should not give a warning if the symbol is
a macro argument. parse_define(), in tccpp.c, indicates that macro arguments are marked
with SYM_FIELD. That bit is set with the "val" token index in the warning above. It may
make sense to suppress that warning if the symbol is a SYM_FIELD.

Having fixed that, I still have the error of too many args. I turn my attention to
parse_define, defined in tccpp.c. During argument processing, sym_push2 gets the new
symbol pointer. If sym_push2 returns a null pointer for extended preprocessor symbols,
then the argument list will expect zero arguments. Is this the case?

The function sym_push2 is defined in tccgen.c. It always pushes the symbol onto the
given symbol stack (which, in this particular case, is the define symbol stack)...

#### September 7, 2014

Aha! Preprocessor macro definitions need to do this because each of the macro arguments
needs to be able to link to the next macro argument. Thus, each argument needs its own
Symbol entry, apart from any TokenSym. The problem, however, is that my
get_new_deftab_pointer in libtcc.c assumes that it can lookup any symbol by finding its
TokenSym container, and checking the TokenSym's sym_define field. This clearly won't work
correctly for macro names used by multiple extended symbol tables, which is the case being
triggered by test 28.

To solve this properly, I think I need to be able to lookup a define stack sym by its
original address. This I can do by having a two-stage copy process: the first stage copies
most of the information, including the token stream, as well as the original Sym's address
into the (presently unused) .prev field of the new Sym struct. The second stage sets the
.next field, calling a reworked get_new_deftab_pointer, which identifies the new Sym by
matching against the addresses in the .prev field.

#### September 8, 2014

It turns out that the solution was even simpler than this: I simply needed to disable a
TokenSym check that had been in place. That was my attempt to solve a problem that can
never happen, and causing a bug in the process. Having removed that, test 28 now, finally,
passes! HURRAY!

For better or worse, however, a warning that I promoted to an error is now causing a
number of tests in the suite to fail. These include:

test 20: Cannot use name 'l' as a global variable, it is already in the extended symbol table.
test 30: Cannot use name 'pt' as a global variable, it is already in the extended symbol table.
test 35: Cannot use name 'pt' as a global variable, it is already in the extended symbol table.

And then, of course, test 70 still exhibits three failed tests, as already documented.

What is going on for these newly failing tests? For test 20, the token 'l' is not a global
variable at all but simply the name of a function argument. The same is true for 'pt' in
tests 30 and 35. Furthermore, in tests 30 and 35, 'pt' is the name of a local variable.
Even if I sort out the function argument issue, this ultimately means that the use of a
*local* variable name in an extended symbol table precludes the use of global variable by
the same name.

As such, I am beginning to believe that extended symbol tables need to be able to restore
their TokenSyms to pristine states after being used by a dependent compilation unit. This
leads, further, to the ability for a dependent extended symbol table to re-initialize the
modified TokenSyms, so that compilation units further down the chain get the full, proper
behavior. (Ultimately, this may lead to a sort of diamond inheritance problem, but I
dearly hope it is not an issue in actual practice.)

And we have a currently unused slot in the copied TokenSyms: the .hash_next field.

---

It would be nice if I could simply construct a linear MRO. Then I could order the extended
symbol tables and look up extended symbols. In that case, adding a function using the same
token as a previously defined (say) preprocessor macro argument would simply copy the old
extended TokenSym into the new extended symbol table. Because it would be asked before
the parent extended symbol table, it would be able to supply the modified answer. A simple
copy-on-write handling for extended TokenSyms would make it fairly easy implement. The
problem is that I don't know how to write a C3 linearization.

The alternative is to have each dependent extended symbol table keep track of its
modifications. When you load an extended symbol table, and then another one, the dependent
symbol table would insert temporary modifications into the original before compilation
begins. After compilation, it would then clean up any changes. This is complicated.

So, I think that C3 might be the best option, really.

---

Let's take a stop back. Why is this even a discussion? A global function can only be
declared once, of course, but argument names (apparently) go on the global stack, maybe,
and preprocessor macros can be #defined and #undef'd repeatedly. If a user pulls in
multiple extended symbol tables that redefine a preprocessor macro, then I need some way
of picking which one. This is a classic dependency calculation, and C3 is a classic
solution to this problem.


November 6, 2014
================

After a long hiatus, I was thinking again about this project. I stopped on a
snag with the symbol table. I cannot simply loan a parent symbol table to a
consuming compilation unit because that compilation unit could modify the
original symbol table. The guts underlying tcc assumes that there is only one
symbol table, and it therefore directly modifies the data structures.

The idea that occurred to me today is that perhaps I could copy the symbols at
the beginning of the compilation unit. The system would emulate the @EXPORT
model of sharing stuff... It takes only a moment's thought to remember that the
whole advantage of symbol table sharing in the first place is to avoid copying
enormous tables (i.e. Perl's C API) to every single compilation unit... damn.

But, I had another idea this evening: whenever an extended TokenSym is looked up
by name, the TokenSym struct and all of its elements should be copied to a local
TokenSym. This would happen somewhere in the vicinity of the call to
symtab_name_callback in tccpp.c. This way, you only pay for the tokensyms that
you use. This would have the unfortunate side effect that regularly used tokens
that happen to exist in the Perl C API would get copied a lot, but I'm pretty
sure it'll solve the problem and let me progress, and sub-optimal progress is
better than none at all.

In that case, I would no longer need to use the extended token flag in the tok
identifier in the way that I currently do (i.e. to sequentially list all symbols
ever seen by tcc throughout the entire process). The primary advantage of that
old system was that I had separate callbacks for retrieving identifiers, which
was a signal that pointers to those identifiers needed to be added to the
compilation unit. How do I keep that functionality?

One idea is to use the flag to mean "This is a copied extended token." Usually,
this flag will be stripped off, but in tccgen.c:sym_find, it would be a
notification that the symbol needs to be added to the compilaton unit. A
callback would be fired. Then, the TokenSym's tok field would be modified to
remove the extended symbol flag. Future calls to sym_find would check for the
extended symbol flag in the TokenSym's tok, and would not fire the callback if
the extended symbol flag were not present.

November 17, 2014
=================

Progress slow but steady. At this point, I have cleaned out the code that checks
for extended symbols in symbol lookups. The only remaining bit is to ensure that
new tokensym lookups into extended symbol tables end up creating and copying
new tokensyms. The heart of what needs to happen is simple enough. During the
token lookup in tccpp.c's next_nomacro1, we check if a given bareword exists in
an extended symbol table. If it does, then a new token in the current compiling
context is created which is a clone of the one in the extended symbol table.
This involves a copy of the define Sym, the struct Sym, and the identifier Sym,
along with all of their dependents.

Dependents? Yes, preprocessor definitions include token streams, i.e. streams of
integers that give the series of tokens for the preprocessor macro expansion.
Each one of these tokens needs to be copied to the new compiler context, and the
token stream needs to be updated from the old to the new token numbers. The
types of struct members and the arguments of functions can be structs, function
pointers, or some sort of typedef; these members and arguments also need to be
copied to the new compiler context and their token numbers updated.

In order to avoid unnecessary copying, it is important to check if a tokensym
has already been created for the dependent token. To do this, I need to check
the currently compiling context if it has a token by the same name, which is the
purpose of tok_alloc. In fact, if such a token does not yet exist, this function
will also allocate it for me, which will let me continue with a recursive copy
procedure. That's almost perfect, except that if it *does* exist, I don't want
to recursively copy, and there's no way for me to know if the returned TokenSym
needs to be copied or not. So I guess I'll have to perform the hash lookup
manually.

November 22, 2014
=================

After a lot of work, I believe I have my first shot at a re-implementation. The
final solution involves a lot of local and extended symbol table lookups, and I
am beginning to wonder if there could be a way to avoid repeated hashing of the
same character string during the extended symbol lookup. For now, I will not
worry about it, but I will make a mental bookmark that this is a potential
optimization during the copy procedure.

I actually completed this work yesterday. I have not even tried to compile it
(let alone test it), so today I will commit this work, perform a wash-rinse-
repeat until I've cleared out all of the C language typos, and then retool the
lookup code in the test suite to see if the logic (and my understanding of what
the compiler needs) is right.

November 23, 2014
=================

Having cleaned up the typos, tcc compiles again. Furthermore, as best I can
tell, it passes its usual (non-extended-symtab) test suite. Cool.

Now I need to see how it behaves with the extended symbol table test suite,
which means it's time to revise that test suite to use the new API. Even before
doing that, tests 01 and 10 passed. However, in order to get test 20 to pass, I
had to revise the contents of test_setup.h.

And now, test 20 fails after the eighth test stating:
  <string>:1: error: Internal error in extended symtab copy: token in define stream was less than tok_start
or, with a minor revision to the way that error gets reported:
  <string>:1: error: Internal error in extended symtab copy: token in define stream (28) was less than tok_start

This suggests thar there is a problem with the stream copy procedure. How could
this arise? To shed some light, I added another piece to the diagnostic, giving
  <string>:1: error: Internal error in extended symtab copy: token in define stream (28) was less than tok_start (40000000)

that means tok_start IS the first extended tokensym value, which it should not
be! This led me to an erroneous setting of the tok field for TokenSyms in
copy_extended_symtab, now fixed. That gives me a new error:
  <string>:1: error: Internal error in extended symtab copy: token in define stream (28) was less than tok_start (46F)

Now that tok_start looks right, what can I say about the token in the stream? It
seems much too small. It's the 40th token symbol. This problem arises with the
preprocessor definition:
  #define MAX(first, second) (first < second ? second : first)

My hunch is that such tokens *are* allowed, that they are just the low-value
tokens. In fact, the ASCII value 40 is the character '('!!! So it looks like it
is simply trying to copy this ASCII value. This means that token values less
than tok_start should be copied as-is, and only values larger than or equal to
tok_start should be modified.

Having fixed that, I now get a segfault after test 8 passes. Perhaps this is an
issue with the extended symbol data not being initialized?

No, unfortunately. Even with the correct initialization, the system still
segfaults. Further digging is required, more than I can spare at the moment.

November 24, 2014
=================

Making test 20 with the .dbg extension (what a wonderful tool), I see that I get
a segfault at tccpp.c:2296. Printing out the contents of local_ts, I get
  {hash_next = 0x0, sym_define = 0x0, sym_label = 0x0, sym_struct = 0x0,
    sym_identifier = 0x0, tok = 1073742963, len = 5, str = "f"}

It becomes quite clear that the local_ts is fine, but the access to
local_ts->sym_define->type.t is clearly erroneous. After all, this should be
*setting* the local tokensym's sym_define's type.t, not accessing it. This
should be curr_from_arg->type.t.

AND THAT FIXES TEST 20!!!! HURRAY!

And with fixes to SETUP_SECOND_CALLBACK_DATA, test 25 passes, too. :-)

Test 28 has its own by-name and by-number lookups that must be revised. Having
revised those, I get core dumps at line 2272 of tccpp.c. A quick debugging
session reveals that at least one problem lies with the value from the from
stream at len: it still has the extended symbol flag.

I still hit trouble, but the problem now appears to be in test_setup.h. A stack
trace at the new segfault indicates that there is call to sym_used, which itself
calls tcc_get_symbol with a compiler state (already compiled) of NULL! This
means that the wrapping code in the test suite is failing somehow. Of course! I
never set up the state pointers in the callback struct! And just like that, test
28 passes!

Applying identical fixups to test 29 fixes that test suite as well.

Test 30 hits a new bug:
  <string>:2: error: field not found: x

This means that the struct definition for the point struct was incorrectly
copied to the second compiler context. Following in the footsteps of
investigations from late April (found around line 120 of this document), I
placed this print statement at the end of copy_extended_tokensym:
  printf("Just copied token with string %s\n", from->str);

I see a message that clearly indicates that the token string 'x' was copied.
The next logical step would be whether the field is correctly added to the
struct Sym's list of fields, which is performed by the copy_extended_sym
function just below. I added a couple of print statements to the anonymous
and non-anonymous "next" copy blocks in that block, i.e. 
  printf("** next points to anonymous symbol with token id %X\n", next->v);
and
  printf("** next points to token id %X, named %s\n", next->v, orig_ts->str);
in the "if (next->v | SYM_FIRST_ANOM)" and "else" blocks, respectively. The very
surprising result is that I get the first printf, but next->v does not look
right! That is, I get
  ** next points to anonymous symbol with token id 10000472
The problem here is that SYM_FIRST_ANOM is defined in tcc.h as 0x08000000, which
explicitly does NOT show up in this list of bits! How then does it get through
the if block and into this chunk of code? Argh! Another print statement made it
obvious. This code:
  printf("** masked, that comes to %X\n", next->v | SYM_FIRST_ANOM);
led to this output:
  ** masked, that comes to 18000472
And it becomes quite obvious I want a bitwise AND, not bitwise OR!

With that fix, I get to a coredump, without any prints. Trying to add a flush
leads to no print, and I quickly realize that the array lookup for orig_ts fails
to mask out flags. That is easily fixed, leading me back to my original
field-not-found problem.

The next step will be to examine how struct fields are set up and stored, and to
set a dbg breakpoint on tccpp.c:2354 to step through how my fields are copied.
